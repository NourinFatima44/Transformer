{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioBasedTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCOlil1jp3Ts"
      },
      "source": [
        "Drive Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Yc4PnPODA-",
        "outputId": "3f14fc9a-9ba0-4930-9c69-d1bfb59ba1a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y5Fs7SwsapO"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqQSW-NGIXF"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "!pip install librosa\n",
        "import librosa\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "          \n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqwgPFtdsiqE"
      },
      "source": [
        "Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rezYH_TMN_q-",
        "outputId": "15da2ed3-5500-4041-fc26-d6e5256e46bc"
      },
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv(\"/content/drive/My Drive/coswara.csv\")\n",
        "from sklearn.utils import shuffle\n",
        "#suffle the dataset\n",
        "data = shuffle(data)\n",
        "data=data.transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GNvlIRON_q_"
      },
      "source": [
        "# Pick up the labels and data\n",
        "X = data.drop(data[[67415]], axis=1)\n",
        "y=data[[67415]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-7kFlxMFA2x"
      },
      "source": [
        "#filling missing value if any row has\n",
        "X.fillna(0, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iQBwA5sg80q"
      },
      "source": [
        "#Preprocessing\n",
        "scaler = StandardScaler()\n",
        "import numpy as np\n",
        "X = scaler.fit_transform(np.array(X, dtype=np.float32))\n",
        "#X=np.array(X, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMIDuPkChGj2"
      },
      "source": [
        "#coverting label into numeirc\n",
        "encoder1 = LabelEncoder()\n",
        "Y = encoder1.fit_transform(y)\n",
        "#Y=np.array(Y,dtype=float)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkvjuOQfBwlX"
      },
      "source": [
        "#applied PCA having a varience of 90%\n",
        "pca = PCA()\n",
        "principal_components12 = pca.fit_transform(X)\n",
        "principal_df12 = pd.DataFrame(data = principal_components12)\n",
        "principal_df12.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42CLwxIElnIQ"
      },
      "source": [
        "#feature selection\n",
        "#from sklearn.feature_selection import SelectKBest, chi2\n",
        "#X_new = SelectKBest(chi2, k=20).fit_transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92QlNNlVtCix"
      },
      "source": [
        "Transfomer coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMPHUERZFXzg"
      },
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask zero out padding tokens.\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    return tf.matmul(attention_weights, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWNx4t8OqdSJ"
      },
      "source": [
        "MultiHeadAttention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F4_iB_KFXzj"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 1,2,3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "        # linear layers\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # split heads\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 1,2,3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9SYrwnHqiKd"
      },
      "source": [
        "Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsfTXzebFXzn"
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model)\n",
        "        # apply sin to even index in the array\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        # apply cos to odd index in the array\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAzUeXvVqm9a"
      },
      "source": [
        "Padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pq5OKITFXzz"
      },
      "source": [
        "# This allows to the transformer to know where there is real data and where it is padded\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bff6djgrqr7v"
      },
      "source": [
        "Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U389WZgtFXzq"
      },
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEbBpxqFXzt"
      },
      "source": [
        "def encoder(time_steps,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            projection,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "  \n",
        "    if projection=='linear':\n",
        "        projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
        "        print('linear')\n",
        "  \n",
        "    else:\n",
        "        projection=tf.identity(inputs)\n",
        "        print('none')\n",
        "   \n",
        "    projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    projection = PositionalEncoding(time_steps, d_model)(projection)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "            )([outputs, padding_mask])\n",
        " \n",
        " \n",
        "  \n",
        "\n",
        " \n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnqhzJThqzMm"
      },
      "source": [
        "\n",
        "transformer main code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uLLTcPFXzw"
      },
      "source": [
        "def transformer(time_steps,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                output_size,\n",
        "                projection,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
        "  \n",
        "  \n",
        "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name='enc_padding_mask')(tf.dtypes.cast(\n",
        "          \n",
        "    #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
        "    # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked      \n",
        "    tf.math.reduce_sum(\n",
        "    inputs,\n",
        "    axis=2,\n",
        "    keepdims=False,\n",
        "    name=None\n",
        "), tf.float32))\n",
        "  \n",
        "\n",
        "    enc_outputs = encoder(\n",
        "      time_steps=time_steps,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      projection=projection,\n",
        "      name='encoder')(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    #We reshape for feeding our FC in the next step\n",
        "    outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
        "  \n",
        "    #We predict our class\n",
        "    outputs = tf.keras.layers.Dense(units=output_size,use_bias=True,activation='sigmoid', name=\"outputs\")(outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX17h1D3q5Av"
      },
      "source": [
        "Spliting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU4hWuxYFX0C"
      },
      "source": [
        "#We get our train and test set\n",
        "X_train,X_test, Y_train, Y_test =train_test_split(principal_df12,Y, test_size=0.2, random_state=27)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_5HZZ4RN_rG"
      },
      "source": [
        "#dimesnion expanding because it required 3D but we are deailting with 2D\n",
        "X_test= np.expand_dims(X_test, 1)\n",
        "X_train= np.expand_dims(X_train, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1YOoBybN_rH"
      },
      "source": [
        "principal_df12= np.expand_dims(principal_df12, 1)\n",
        "#X= np.expand_dims(X, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXGTtytLrHTT"
      },
      "source": [
        "#just for number of head it required if 0< then we required"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcSAKnbON_rI"
      },
      "source": [
        "#projection=['linear','none']\n",
        "projection=['linear']\n",
        "accuracy=[]\n",
        "proj_implemented=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yLFA19WrZbR"
      },
      "source": [
        "Model for COVID-19\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ifMZKEFX0J"
      },
      "source": [
        "\n",
        "for i in projection:\n",
        "    NUM_LAYERS = 2\n",
        "    D_MODEL = principal_df12.shape[2]\n",
        "    NUM_HEADS = 1\n",
        "    UNITS = 1024\n",
        "    DROPOUT = 0.1\n",
        "    TIME_STEPS= principal_df12.shape[1]\n",
        "    OUTPUT_SIZE=1\n",
        "    EPOCHS = 100\n",
        "    EXPERIMENTS=1\n",
        "     #num_layers = 4\n",
        "    #d_model = 128\n",
        "    #dff = 512\n",
        "    #num_heads = 8\n",
        "    #division   dropout_rate = 0.1\n",
        "    for j in range(EXPERIMENTS):\n",
        "        model = transformer(time_steps=TIME_STEPS,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        units=UNITS,\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dropout=DROPOUT,\n",
        "        output_size=OUTPUT_SIZE,  \n",
        "        projection=i)\n",
        "    \n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(0.000001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        history=model.fit(X_train,Y_train, epochs=EPOCHS, validation_split=0.15)\n",
        "        accr = model.evaluate(X_test,Y_test)\n",
        "        print(accr)\n",
        "        accuracy.append(max(history.history['val_accuracy']))\n",
        "        proj_implemented.append(i)\n",
        "        import matplotlib.pyplot as plt\n",
        "        acc = history.history['accuracy']\n",
        "        val_acc = history.history['val_accuracy']\n",
        "        loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "\n",
        "        epochs = range(1, len(acc) + 1)\n",
        "\n",
        "        # \"bo\" is for \"blue dot\"\n",
        "        plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "\n",
        "\n",
        "        # b is for \"solid blue line\"\n",
        "        plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "        plt.title('Training and validation loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()\n",
        "        plt.clf()   # clear figure\n",
        "        history_dict = history.history\n",
        "\n",
        "        print(history_dict.keys())\n",
        "        acc_values = history_dict['accuracy']\n",
        "        val_acc_values = history_dict['val_accuracy']\n",
        "        plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "        plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "        plt.title('Training and validation accuracy')\n",
        "        plt.xlabel('Epochs')  \n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()\n",
        "        import matplotlib.pyplot as plt\n",
        "        yhat_classes = model.predict(X_test,verbose=0)\n",
        "                   \n",
        "# predict crisp classes for test set\n",
        "        #yhat_classes = model.predict_classes(X_train,verbose=0)\n",
        "        #print(yhat_classes)\n",
        "        #yhat_classes=yhat_classes.astype('int32')\n",
        "        #yhat_classes=yhat_classes.reshape(-1,1)\n",
        "        #yhat_probs = model.predict(X_test)\n",
        "        #print(yhat_probs.round())\n",
        "        #yhat_probs=yhat_probs.round()\n",
        "        #yhat_classes=yhat_classes.reshape(-1,1)\n",
        "    \n",
        "\n",
        "        # accuracy: (tp + tn) / (p + n)\n",
        "        accuracy = accuracy_score(Y_test, yhat_classes.round())\n",
        "        print('Accuracy: %f' % accuracy)\n",
        "        # precision tp / (tp + fp)\n",
        "        precision = precision_score(Y_test, yhat_classes.round())\n",
        "        print('Precision: %f' % precision)\n",
        "        # recall: tp / (tp + fn)\n",
        "        recall = recall_score(Y_test, yhat_classes.round())\n",
        "        print('Recall: %f' % recall)\n",
        "        # f1: 2 tp / (2 tp + fp + fn)\n",
        "        f1 = f1_score(Y_test, yhat_classes.round())\n",
        "        print('F1 score: %f' % f1)\n",
        "\n",
        "        # confusion matrix\n",
        "        matrix = confusion_matrix(Y_test, yhat_classes.round())\n",
        "        print(matrix)\n",
        "        import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "        del model\n",
        "        del history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chxvhHsAFX02"
      },
      "source": [
        "accuracy=pd.DataFrame(accuracy, columns=['accuracy'])\n",
        "proj_implemented=pd.DataFrame(proj_implemented, columns=['projection'])\n",
        "results=pd.concat([accuracy,proj_implemented],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LJeC1iwccFA"
      },
      "source": [
        "results.groupby('projection').mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}